#!/usr/bin/env python3
"""
Discourse Security Scanner - Passive Scanning Module

Performs passive reconnaissance without aggressive probing
"""

import re
import time
import json
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from ...lib.discourse_utils import make_request

class PassiveScannerModule:
    """Passive scanning module for Discourse forums"""

    def __init__(self, scanner):
        self.scanner = scanner
        self.results = {
            'module_name': 'Passive Scanner',
            'target': scanner.target_url,
            'discourse_info': {},
            'server_info': {},
            'technology_stack': {},
            'exposed_endpoints': [],
            'meta_information': {},
            'security_headers': {},
            'robots_txt': {},
            'sitemap_info': {},
            'dns_info': {},
            'ssl_info': {},
            'scan_time': 0
        }
        self.start_time = time.time()

    def run(self):
        """Run passive scanner module"""
        self.scanner.log("Starting passive reconnaissance...")

        # Gather basic information
        self._gather_discourse_info()

        # Analyze server headers
        self._analyze_server_headers()

        # Check security headers
        self._check_security_headers()

        # Analyze robots.txt
        self._analyze_robots_txt()

        # Check sitemap
        self._check_sitemap()

        # Gather meta information
        self._gather_meta_info()

        # Detect technology stack
        self._detect_technology_stack()

        # Find exposed endpoints
        self._find_exposed_endpoints()

        # SSL/TLS analysis
        self._analyze_ssl_info()

        self.results['scan_time'] = time.time() - self.start_time
        return self.results

    def _gather_discourse_info(self):
        """Gather basic Discourse information passively"""
        self.scanner.log("Gathering Discourse information...", 'debug')

        # Check main page
        response = self.scanner.make_request(self.scanner.target_url)
        if response:
            soup = BeautifulSoup(response.text, 'html.parser')

            # Extract Discourse version from meta tags
            generator = soup.find('meta', {'name': 'generator'})
            if generator:
                content = generator.get('content', '')
                if 'discourse' in content.lower():
                    version_match = re.search(r'discourse\s+([\d\.]+)', content, re.IGNORECASE)
                    if version_match:
                        self.results['discourse_info']['version'] = version_match.group(1)

            # Extract site title
            title = soup.find('title')
            if title:
                self.results['discourse_info']['site_title'] = title.text.strip()

            # Extract description
            description = soup.find('meta', {'name': 'description'})
            if description:
                self.results['discourse_info']['description'] = description.get('content', '')

        # Check about.json endpoint
        about_url = urljoin(self.scanner.target_url, '/about.json')
        response = self.scanner.make_request(about_url)
        if response and response.status_code == 200:
            try:
                about_data = response.json()
                self.results['discourse_info']['about'] = about_data
                if 'version' in about_data:
                    self.results['discourse_info']['version'] = about_data['version']
            except:
                pass

    def _analyze_server_headers(self):
        """Analyze server response headers"""
        self.scanner.log("Analyzing server headers...", 'debug')

        response = self.scanner.make_request(self.scanner.target_url)
        if response:
            headers = dict(response.headers)

            # Extract server information
            server = headers.get('Server', '')
            if server:
                self.results['server_info']['server'] = server

            # Extract powered-by information
            powered_by = headers.get('X-Powered-By', '')
            if powered_by:
                self.results['server_info']['powered_by'] = powered_by

            # Extract other interesting headers
            interesting_headers = [
                'X-Frame-Options',
                'X-Content-Type-Options',
                'X-XSS-Protection',
                'Strict-Transport-Security',
                'Content-Security-Policy',
                'X-Discourse-Route',
                'X-Discourse-Username',
                'X-Runtime'
            ]

            for header in interesting_headers:
                if header in headers:
                    self.results['server_info'][header.lower().replace('-', '_')] = headers[header]

    def _check_security_headers(self):
        """Check security-related headers with enhanced analysis"""
        self.scanner.log("Checking security headers...", 'debug')

        response = self.scanner.make_request(self.scanner.target_url)
        if response:
            headers = dict(response.headers)

            security_headers = {
                'X-Frame-Options': {
                    'description': 'Clickjacking protection',
                    'severity': 'high',
                    'secure_values': ['DENY', 'SAMEORIGIN']
                },
                'X-Content-Type-Options': {
                    'description': 'MIME type sniffing protection',
                    'severity': 'medium',
                    'secure_values': ['nosniff']
                },
                'X-XSS-Protection': {
                    'description': 'XSS protection (deprecated but still useful)',
                    'severity': 'low',
                    'secure_values': ['1; mode=block']
                },
                'Strict-Transport-Security': {
                    'description': 'HTTPS enforcement',
                    'severity': 'critical',
                    'secure_values': ['max-age=']
                },
                'Content-Security-Policy': {
                    'description': 'Content injection protection',
                    'severity': 'critical',
                    'secure_values': ["default-src 'self'"]
                },
                'Referrer-Policy': {
                    'description': 'Referrer information control',
                    'severity': 'medium',
                    'secure_values': ['strict-origin-when-cross-origin', 'no-referrer']
                },
                'Permissions-Policy': {
                    'description': 'Feature policy control',
                    'severity': 'medium',
                    'secure_values': ['geolocation=', 'microphone=', 'camera=']
                },
                'Cross-Origin-Embedder-Policy': {
                    'description': 'Cross-origin isolation',
                    'severity': 'medium',
                    'secure_values': ['require-corp']
                },
                'Cross-Origin-Opener-Policy': {
                    'description': 'Cross-origin window isolation',
                    'severity': 'medium',
                    'secure_values': ['same-origin']
                },
                'Cross-Origin-Resource-Policy': {
                    'description': 'Cross-origin resource sharing control',
                    'severity': 'medium',
                    'secure_values': ['same-origin', 'cross-origin']
                }
            }

            for header, config in security_headers.items():
                if header in headers:
                    header_value = headers[header]
                    is_secure = self._analyze_header_security(header, header_value, config['secure_values'])
                    
                    self.results['security_headers'][header] = {
                        'present': True,
                        'value': header_value,
                        'description': config['description'],
                        'severity': config['severity'],
                        'is_secure': is_secure,
                        'analysis': self._get_header_analysis(header, header_value)
                    }
                else:
                    self.results['security_headers'][header] = {
                        'present': False,
                        'description': config['description'],
                        'severity': config['severity'],
                        'risk': f'Missing {config["severity"]} security header',
                        'recommendation': self._get_header_recommendation(header)
                    }

    def _analyze_header_security(self, header_name, header_value, secure_values):
        """Analyze if header value is secure"""
        header_lower = header_value.lower()
        
        for secure_val in secure_values:
            if secure_val.lower() in header_lower:
                return True
        return False

    def _get_header_analysis(self, header_name, header_value):
        """Get detailed analysis for specific headers"""
        analysis = []
        
        if header_name == 'Strict-Transport-Security':
            if 'max-age=' in header_value:
                try:
                    max_age = int(header_value.split('max-age=')[1].split(';')[0])
                    if max_age < 31536000:  # Less than 1 year
                        analysis.append('HSTS max-age is less than recommended 1 year')
                    if 'includeSubDomains' not in header_value:
                        analysis.append('HSTS should include subdomains')
                    if 'preload' not in header_value:
                        analysis.append('Consider adding preload directive')
                except ValueError:
                    analysis.append('Invalid max-age value')
        
        elif header_name == 'Content-Security-Policy':
            if "'unsafe-inline'" in header_value:
                analysis.append('CSP allows unsafe-inline which reduces security')
            if "'unsafe-eval'" in header_value:
                analysis.append('CSP allows unsafe-eval which reduces security')
            if 'data:' in header_value:
                analysis.append('CSP allows data: URIs which may be risky')
            if "default-src 'none'" not in header_value and "default-src 'self'" not in header_value:
                analysis.append('CSP should have restrictive default-src')
        
        elif header_name == 'X-Frame-Options':
            if header_value.upper() not in ['DENY', 'SAMEORIGIN']:
                analysis.append('X-Frame-Options should be DENY or SAMEORIGIN')
        
        return analysis if analysis else ['Header configuration appears secure']

    def _get_header_recommendation(self, header_name):
        """Get recommendation for missing headers"""
        recommendations = {
            'X-Frame-Options': 'Add "X-Frame-Options: DENY" or "X-Frame-Options: SAMEORIGIN"',
            'X-Content-Type-Options': 'Add "X-Content-Type-Options: nosniff"',
            'X-XSS-Protection': 'Add "X-XSS-Protection: 1; mode=block"',
            'Strict-Transport-Security': 'Add "Strict-Transport-Security: max-age=31536000; includeSubDomains; preload"',
            'Content-Security-Policy': 'Add "Content-Security-Policy: default-src \'self\'; script-src \'self\'; style-src \'self\'"',
            'Referrer-Policy': 'Add "Referrer-Policy: strict-origin-when-cross-origin"',
            'Permissions-Policy': 'Add "Permissions-Policy: geolocation=(), microphone=(), camera=()"',
            'Cross-Origin-Embedder-Policy': 'Add "Cross-Origin-Embedder-Policy: require-corp"',
            'Cross-Origin-Opener-Policy': 'Add "Cross-Origin-Opener-Policy: same-origin"',
            'Cross-Origin-Resource-Policy': 'Add "Cross-Origin-Resource-Policy: same-origin"'
        }
        return recommendations.get(header_name, 'Configure this security header appropriately')

    def _analyze_robots_txt(self):
        """Analyze robots.txt file"""
        self.scanner.log("Analyzing robots.txt...", 'debug')

        robots_url = urljoin(self.scanner.target_url, '/robots.txt')
        response = self.scanner.make_request(robots_url)

        if response and response.status_code == 200:
            robots_content = response.text
            self.results['robots_txt']['content'] = robots_content

            # Extract disallowed paths
            disallow_pattern = r'Disallow:\s*(.+)'
            disallowed = re.findall(disallow_pattern, robots_content, re.IGNORECASE)
            self.results['robots_txt']['disallowed_paths'] = disallowed

            # Extract allowed paths
            allow_pattern = r'Allow:\s*(.+)'
            allowed = re.findall(allow_pattern, robots_content, re.IGNORECASE)
            self.results['robots_txt']['allowed_paths'] = allowed

            # Extract sitemap references
            sitemap_pattern = r'Sitemap:\s*(.+)'
            sitemaps = re.findall(sitemap_pattern, robots_content, re.IGNORECASE)
            self.results['robots_txt']['sitemaps'] = sitemaps
        else:
            self.results['robots_txt']['status'] = 'Not found'

    def _check_sitemap(self):
        """Check sitemap files"""
        self.scanner.log("Checking sitemap...", 'debug')

        sitemap_urls = [
            '/sitemap.xml',
            '/sitemap_index.xml',
            '/sitemaps/sitemap.xml'
        ]

        for sitemap_path in sitemap_urls:
            sitemap_url = urljoin(self.scanner.target_url, sitemap_path)
            response = self.scanner.make_request(sitemap_url)

            if response and response.status_code == 200:
                self.results['sitemap_info'][sitemap_path] = {
                    'found': True,
                    'size': len(response.content),
                    'content_type': response.headers.get('content-type', 'unknown')
                }

                # Extract URLs from sitemap
                url_pattern = r'<loc>([^<]+)</loc>'
                urls = re.findall(url_pattern, response.text)
                self.results['sitemap_info'][sitemap_path]['urls'] = urls[:50]  # Limit to first 50
            else:
                self.results['sitemap_info'][sitemap_path] = {'found': False}

    def _gather_meta_info(self):
        """Gather meta information from HTML"""
        self.scanner.log("Gathering meta information...", 'debug')

        response = self.scanner.make_request(self.scanner.target_url)
        if response:
            soup = BeautifulSoup(response.text, 'html.parser')

            # Extract all meta tags
            meta_tags = soup.find_all('meta')
            for meta in meta_tags:
                name = meta.get('name') or meta.get('property') or meta.get('http-equiv')
                content = meta.get('content')
                if name and content:
                    self.results['meta_information'][name] = content

            # Extract JavaScript files
            scripts = soup.find_all('script', {'src': True})
            js_files = [script['src'] for script in scripts]
            self.results['meta_information']['javascript_files'] = js_files[:20]  # Limit to first 20

            # Extract CSS files
            links = soup.find_all('link', {'rel': 'stylesheet'})
            css_files = [link['href'] for link in links if link.get('href')]
            self.results['meta_information']['css_files'] = css_files[:20]  # Limit to first 20

    def _detect_technology_stack(self):
        """Detect technology stack passively"""
        self.scanner.log("Detecting technology stack...", 'debug')

        response = self.scanner.make_request(self.scanner.target_url)
        if response:
            headers = dict(response.headers)
            content = response.text

            # Detect web server
            server = headers.get('Server', '')
            if server:
                if 'nginx' in server.lower():
                    self.results['technology_stack']['web_server'] = 'Nginx'
                elif 'apache' in server.lower():
                    self.results['technology_stack']['web_server'] = 'Apache'
                elif 'cloudflare' in server.lower():
                    self.results['technology_stack']['cdn'] = 'Cloudflare'

            # Detect Ruby on Rails (Discourse is built on Rails)
            if 'X-Runtime' in headers or 'rails' in content.lower():
                self.results['technology_stack']['framework'] = 'Ruby on Rails'

            # Detect Redis (commonly used with Discourse)
            if 'redis' in content.lower():
                self.results['technology_stack']['cache'] = 'Redis'

            # Detect PostgreSQL (Discourse's database)
            if 'postgresql' in content.lower() or 'postgres' in content.lower():
                self.results['technology_stack']['database'] = 'PostgreSQL'

            # Detect JavaScript frameworks
            if 'ember' in content.lower():
                self.results['technology_stack']['frontend'] = 'Ember.js'
            if 'jquery' in content.lower():
                self.results['technology_stack']['javascript_library'] = 'jQuery'

    def _find_exposed_endpoints(self):
        """Find exposed endpoints passively"""
        self.scanner.log("Finding exposed endpoints...", 'debug')

        # Common Discourse endpoints to check passively
        endpoints = [
            '/about.json',
            '/site.json',
            '/categories.json',
            '/latest.json',
            '/top.json',
            '/users.json',
            '/groups.json',
            '/badges.json',
            '/tags.json',
            '/search.json',
            '/admin',
            '/admin/dashboard',
            '/admin/users',
            '/login',
            '/signup',
            '/privacy',
            '/tos',
            '/faq'
        ]

        for endpoint in endpoints:
            url = urljoin(self.scanner.target_url, endpoint)
            response = self.scanner.make_request(url)

            if response:
                self.results['exposed_endpoints'].append({
                    'endpoint': endpoint,
                    'url': url,
                    'status_code': response.status_code,
                    'content_type': response.headers.get('content-type', 'unknown'),
                    'size': len(response.content)
                })

    def _analyze_ssl_info(self):
        """Analyze SSL/TLS information with comprehensive security assessment"""
        self.scanner.log("Analyzing SSL information...", 'debug')

        if self.scanner.target_url.startswith('https://'):
            try:
                import ssl
                import socket
                from urllib.parse import urlparse
                from datetime import datetime, timezone

                parsed_url = urlparse(self.scanner.target_url)
                hostname = parsed_url.hostname
                port = parsed_url.port or 443

                # Get SSL certificate info
                context = ssl.create_default_context()
                with socket.create_connection((hostname, port), timeout=10) as sock:
                    with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                        cert = ssock.getpeercert()
                        protocol_version = ssock.version()
                        cipher_suite = ssock.cipher()

                        # Basic certificate information
                        ssl_info = {
                            'subject': dict(x[0] for x in cert['subject']),
                            'issuer': dict(x[0] for x in cert['issuer']),
                            'version': cert['version'],
                            'serial_number': cert['serialNumber'],
                            'not_before': cert['notBefore'],
                            'not_after': cert['notAfter'],
                            'signature_algorithm': cert.get('signatureAlgorithm', 'unknown'),
                            'protocol_version': protocol_version,
                            'cipher_suite': {
                                'name': cipher_suite[0] if cipher_suite else 'unknown',
                                'version': cipher_suite[1] if cipher_suite else 'unknown',
                                'bits': cipher_suite[2] if cipher_suite else 0
                            }
                        }

                        # Check for SAN (Subject Alternative Names)
                        if 'subjectAltName' in cert:
                            ssl_info['subject_alt_names'] = [name[1] for name in cert['subjectAltName']]

                        # Security analysis
                        ssl_info['security_analysis'] = self._analyze_ssl_security(cert, protocol_version, cipher_suite)
                        
                        # Certificate expiry analysis
                        ssl_info['expiry_analysis'] = self._analyze_certificate_expiry(cert)
                        
                        self.results['ssl_info'] = ssl_info

            except ssl.SSLError as e:
                self.results['ssl_info'] = {
                    'error': f'SSL Error: {str(e)}',
                    'security_analysis': {
                        'overall_rating': 'CRITICAL',
                        'issues': ['SSL/TLS connection failed'],
                        'recommendations': ['Fix SSL/TLS configuration', 'Ensure valid certificate is installed']
                    }
                }
            except socket.timeout:
                self.results['ssl_info'] = {
                    'error': 'Connection timeout during SSL analysis',
                    'security_analysis': {
                        'overall_rating': 'WARNING',
                        'issues': ['SSL handshake timeout'],
                        'recommendations': ['Check server SSL configuration', 'Verify network connectivity']
                    }
                }
            except Exception as e:
                self.results['ssl_info'] = {
                    'error': f'Unexpected error: {str(e)}',
                    'security_analysis': {
                        'overall_rating': 'ERROR',
                        'issues': ['SSL analysis failed'],
                        'recommendations': ['Manual SSL verification recommended']
                    }
                }
        else:
            self.results['ssl_info'] = {
                'status': 'HTTP only - no SSL/TLS',
                'security_analysis': {
                    'overall_rating': 'CRITICAL',
                    'issues': ['No SSL/TLS encryption', 'Data transmitted in plaintext'],
                    'recommendations': ['Implement HTTPS', 'Redirect HTTP to HTTPS', 'Use HSTS headers']
                }
            }

    def _analyze_ssl_security(self, cert, protocol_version, cipher_suite):
        """Analyze SSL/TLS security configuration"""
        issues = []
        recommendations = []
        rating = 'GOOD'
        
        # Check protocol version
        if protocol_version in ['SSLv2', 'SSLv3']:
            issues.append(f'Insecure protocol version: {protocol_version}')
            recommendations.append('Upgrade to TLS 1.2 or higher')
            rating = 'CRITICAL'
        elif protocol_version == 'TLSv1':
            issues.append('Outdated TLS version 1.0')
            recommendations.append('Upgrade to TLS 1.2 or higher')
            rating = 'HIGH' if rating == 'GOOD' else rating
        elif protocol_version == 'TLSv1.1':
            issues.append('Outdated TLS version 1.1')
            recommendations.append('Upgrade to TLS 1.2 or higher')
            rating = 'MEDIUM' if rating == 'GOOD' else rating
        
        # Check cipher suite strength
        if cipher_suite and cipher_suite[2] < 128:
            issues.append(f'Weak cipher strength: {cipher_suite[2]} bits')
            recommendations.append('Use ciphers with at least 128-bit encryption')
            rating = 'HIGH' if rating in ['GOOD', 'MEDIUM'] else rating
        
        # Check signature algorithm
        sig_alg = cert.get('signatureAlgorithm', '').lower()
        if 'md5' in sig_alg:
            issues.append('Weak signature algorithm: MD5')
            recommendations.append('Use SHA-256 or stronger signature algorithm')
            rating = 'CRITICAL'
        elif 'sha1' in sig_alg:
            issues.append('Weak signature algorithm: SHA-1')
            recommendations.append('Use SHA-256 or stronger signature algorithm')
            rating = 'HIGH' if rating in ['GOOD', 'MEDIUM'] else rating
        
        # Check key length (if available in subject)
        subject = dict(x[0] for x in cert['subject'])
        if 'rsaEncryption' in str(cert.get('subjectPublicKeyInfo', '')):
            # This is a simplified check - in practice, you'd need to parse the key properly
            pass
        
        if not issues:
            recommendations.append('SSL/TLS configuration appears secure')
        
        return {
            'overall_rating': rating,
            'protocol_version': protocol_version,
            'cipher_info': cipher_suite,
            'issues': issues,
            'recommendations': recommendations
        }
    
    def _analyze_certificate_expiry(self, cert):
        """Analyze certificate expiry and validity"""
        from datetime import datetime
        
        try:
            # Parse certificate dates
            not_before = datetime.strptime(cert['notBefore'], '%b %d %H:%M:%S %Y %Z')
            not_after = datetime.strptime(cert['notAfter'], '%b %d %H:%M:%S %Y %Z')
            now = datetime.now()
            
            days_until_expiry = (not_after - now).days
            
            status = 'VALID'
            issues = []
            recommendations = []
            
            if now < not_before:
                status = 'NOT_YET_VALID'
                issues.append('Certificate is not yet valid')
                recommendations.append('Check system clock and certificate validity period')
            elif now > not_after:
                status = 'EXPIRED'
                issues.append('Certificate has expired')
                recommendations.append('Renew SSL certificate immediately')
            elif days_until_expiry <= 7:
                status = 'EXPIRING_SOON'
                issues.append(f'Certificate expires in {days_until_expiry} days')
                recommendations.append('Renew SSL certificate soon')
            elif days_until_expiry <= 30:
                status = 'EXPIRING_WITHIN_MONTH'
                recommendations.append(f'Certificate expires in {days_until_expiry} days - consider renewal')
            
            return {
                'status': status,
                'not_before': cert['notBefore'],
                'not_after': cert['notAfter'],
                'days_until_expiry': days_until_expiry,
                'issues': issues,
                'recommendations': recommendations
            }
        except Exception as e:
            return {
                'status': 'ERROR',
                'error': f'Failed to parse certificate dates: {str(e)}',
                'issues': ['Certificate date parsing failed'],
                'recommendations': ['Manual certificate verification recommended']
            }